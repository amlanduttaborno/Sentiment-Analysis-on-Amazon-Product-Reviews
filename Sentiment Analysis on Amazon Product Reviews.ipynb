{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4651dd8e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaf52db",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview\n",
    "- **Dataset Description**:\n",
    "  - Analyze an Amazon product review dataset containing textual reviews (`reviewText`) and corresponding sentiment labels (`Positive`).\n",
    "  - Sentiment is binary: 1 for positive, 0 for negative.\n",
    "- **Objective**:\n",
    "  - Predict the sentiment of a product review based on its textual content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec67bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries and Load Data ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Amazon product reviews dataset\n",
    "# Replace 'amazon_reviews.csv' with your actual file path\n",
    "reviews_df = pd.read_csv('amazon_reviews.csv')\n",
    "\n",
    "# Preview the first few rows\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92730c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a one of the best apps acording to a b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a pretty good version of the game for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a really cool game. there are a bunch ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a silly game and can be frustrating, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  Positive\n",
       "0  This is a one of the best apps acording to a b...         1\n",
       "1  This is a pretty good version of the game for ...         1\n",
       "2  this is a really cool game. there are a bunch ...         1\n",
       "3  This is a silly game and can be frustrating, b...         1\n",
       "4  This is a terrific game on any pad. Hrs of fun...         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Data Exploration ---\n",
    "# Check the shape of the dataset\n",
    "print(f\"Dataset shape: {reviews_df.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(reviews_df.isnull().sum())\n",
    "\n",
    "# Check class distribution (assuming 'sentiment' column exists)\n",
    "if 'sentiment' in reviews_df.columns:\n",
    "    print(\"\\nSentiment class distribution:\")\n",
    "    print(reviews_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27b7a2",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "- Handle missing values, if any.\n",
    "- Perform text preprocessing on the `reviewText` column:\n",
    "  - Convert text to lowercase.\n",
    "  - Remove stop words, punctuation, and special characters.\n",
    "  - Tokenize and lemmatize text data.\n",
    "- Split the dataset into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c497ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Preprocessing ---\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK resources if not already present\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'review' column\n",
    "reviews_df['clean_review'] = reviews_df['review'].apply(preprocess_text)\n",
    "\n",
    "# Preview cleaned reviews\n",
    "reviews_df[['review', 'clean_review']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e83cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exploratory Data Analysis (EDA) ---\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 1. Word Cloud for positive and negative reviews\n",
    "plt.figure(figsize=(10,5))\n",
    "if 'sentiment' in reviews_df.columns:\n",
    "    pos_text = ' '.join(reviews_df[reviews_df['sentiment']=='positive']['clean_review'])\n",
    "    neg_text = ' '.join(reviews_df[reviews_df['sentiment']=='negative']['clean_review'])\n",
    "    wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(pos_text)\n",
    "    wordcloud_neg = WordCloud(width=800, height=400, background_color='black').generate(neg_text)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(wordcloud_pos, interpolation='bilinear')\n",
    "    plt.title('Positive Reviews Word Cloud')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(wordcloud_neg, interpolation='bilinear')\n",
    "    plt.title('Negative Reviews Word Cloud')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 2. Sentiment distribution\n",
    "if 'sentiment' in reviews_df.columns:\n",
    "    sns.countplot(x='sentiment', data=reviews_df)\n",
    "    plt.title('Sentiment Distribution')\n",
    "    plt.show()\n",
    "\n",
    "# 3. Review length analysis\n",
    "reviews_df['review_length'] = reviews_df['clean_review'].apply(lambda x: len(x.split()))\n",
    "sns.histplot(reviews_df['review_length'], bins=30)\n",
    "plt.title('Distribution of Review Lengths')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b6a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Extraction ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF features from cleaned reviews\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(reviews_df['clean_review']).toarray()\n",
    "\n",
    "# Target variable (assuming 'sentiment' column exists)\n",
    "if 'sentiment' in reviews_df.columns:\n",
    "    y = reviews_df['sentiment'].map({'positive':1, 'negative':0})\n",
    "\n",
    "# Preview feature matrix shape\n",
    "print(f\"TF-IDF feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98fa4a",
   "metadata": {},
   "source": [
    "## 3. Model Selection\n",
    "- Choose at least three machine learning models for sentiment classification:\n",
    "  - Statistical Models:\n",
    "    - Logistic Regression\n",
    "    - Random Forest\n",
    "    - Support Vector Machine (SVM)\n",
    "    - Na√Øve Bayes\n",
    "    - Gradient Boosting (e.g., XGBoost, AdaBoost, CatBoost)\n",
    "  - Neural Models:\n",
    "    - LSTM (Long Short-Term Memory)\n",
    "    - GRUs (Gated Recurrent Units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Training ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f1c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Evaluation ---\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_logreg):.2f}\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_logreg))\n",
    "\n",
    "# Evaluate Random Forest\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1edc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization of Model Performance ---\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# ROC Curve for Random Forest\n",
    "y_prob_rf = rf.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_rf)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance for Random Forest\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[-20:][::-1]  # Top 20 features\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(range(len(indices)), importances[indices])\n",
    "plt.xticks(range(len(indices)), [vectorizer.get_feature_names_out()[i] for i in indices], rotation=90)\n",
    "plt.title('Top 20 Important Features (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d561e8",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "- Train each selected model on the training dataset.\n",
    "- Utilize vectorization techniques for text data:\n",
    "  - TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "  - Word embeddings (e.g., Word2Vec, GloVe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict Sentiment on New Reviews ---\n",
    "# Example: Predict sentiment for new/unseen reviews\n",
    "new_reviews = [\n",
    "    \"This product is amazing! Highly recommend.\",\n",
    "    \"Terrible quality, very disappointed.\",\n",
    "    \"Works as expected, good value for money.\"\n",
    "]\n",
    "\n",
    "# Preprocess new reviews\n",
    "new_clean = [preprocess_text(review) for review in new_reviews]\n",
    "new_features = vectorizer.transform(new_clean).toarray()\n",
    "\n",
    "# Predict using Random Forest\n",
    "new_pred = rf.predict(new_features)\n",
    "\n",
    "# Map predictions to labels\n",
    "label_map = {1: 'positive', 0: 'negative'}\n",
    "for review, pred in zip(new_reviews, new_pred):\n",
    "    print(f\"Review: '{review}'\\nPredicted Sentiment: {label_map[pred]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Insights & Recommendations ---\n",
    "# Summarize findings and provide actionable recommendations\n",
    "\n",
    "print(\"\\n--- Insights & Recommendations ---\")\n",
    "if accuracy_score(y_test, y_pred_rf) > 0.85:\n",
    "    print(\"The Random Forest model performs well. Consider deploying it for automated sentiment analysis.\")\n",
    "else:\n",
    "    print(\"Model accuracy is moderate. Try more advanced NLP models or tune hyperparameters.\")\n",
    "\n",
    "# Common positive and negative words\n",
    "if 'sentiment' in reviews_df.columns:\n",
    "    from collections import Counter\n",
    "    pos_words = ' '.join(reviews_df[reviews_df['sentiment']=='positive']['clean_review']).split()\n",
    "    neg_words = ' '.join(reviews_df[reviews_df['sentiment']=='negative']['clean_review']).split()\n",
    "    print(\"\\nTop 10 Positive Words:\", Counter(pos_words).most_common(10))\n",
    "    print(\"Top 10 Negative Words:\", Counter(neg_words).most_common(10))\n",
    "\n",
    "print(\"\\nRecommendation: Monitor product reviews regularly to identify customer pain points and improve product quality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameter Tuning ---\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Random Forest parameter grid\n",
    "grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rf_search = GridSearchCV(RandomForestClassifier(random_state=42), grid_rf, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "rf_search.fit(X_train, y_train)\n",
    "print(f\"Best RF parameters: {rf_search.best_params_}\")\n",
    "print(f\"Best RF cross-validated accuracy: {rf_search.best_score_:.2f}\")\n",
    "\n",
    "# Logistic Regression parameter grid\n",
    "grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "lr_search = GridSearchCV(LogisticRegression(max_iter=1000), grid_lr, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "lr_search.fit(X_train, y_train)\n",
    "print(f\"Best LR parameters: {lr_search.best_params_}\")\n",
    "print(f\"Best LR cross-validated accuracy: {lr_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8c8247",
   "metadata": {},
   "source": [
    "## 5. Formal Evaluation\n",
    "- Evaluate the performance of each model on the testing set using the following metrics:\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1 Score\n",
    "  - Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b3211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Error Analysis ---\n",
    "# Find misclassified examples in test set\n",
    "misclassified_idx = np.where(y_test != y_pred_rf)[0]\n",
    "print(f\"Number of misclassified reviews: {len(misclassified_idx)}\")\n",
    "\n",
    "# Show a few misclassified reviews\n",
    "for i in misclassified_idx[:5]:\n",
    "    print(f\"Review: {reviews_df.iloc[X_test[i].argmax()]['review']}\")\n",
    "    print(f\"True Sentiment: {label_map[y_test.iloc[i]]}, Predicted: {label_map[y_pred_rf[i]]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf38449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Model and Vectorizer ---\n",
    "import joblib\n",
    "\n",
    "# Save Random Forest model\n",
    "joblib.dump(rf, 'sentiment_rf_model.joblib')\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "print(\"Model and vectorizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2900bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Model and Predict on New Data ---\n",
    "import joblib\n",
    "\n",
    "# Load saved Random Forest model and TF-IDF vectorizer\n",
    "loaded_rf = joblib.load('sentiment_rf_model.joblib')\n",
    "loaded_vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "\n",
    "# Example: Predict sentiment for new/unseen reviews\n",
    "new_reviews = [\n",
    "    \"The product exceeded my expectations!\",\n",
    "    \"Not worth the money, very poor quality.\",\n",
    "    \"Average experience, nothing special.\"\n",
    "]\n",
    "\n",
    "# Preprocess new reviews\n",
    "new_clean = [preprocess_text(review) for review in new_reviews]\n",
    "new_features = loaded_vectorizer.transform(new_clean).toarray()\n",
    "\n",
    "# Predict using loaded model\n",
    "new_pred = loaded_rf.predict(new_features)\n",
    "\n",
    "# Map predictions to labels\n",
    "label_map = {1: 'positive', 0: 'negative'}\n",
    "for review, pred in zip(new_reviews, new_pred):\n",
    "    print(f\"Review: '{review}'\\nPredicted Sentiment: {label_map[pred]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcca61",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "- Perform hyperparameter tuning for selected models using:\n",
    "  - Grid Search\n",
    "  - Random Search\n",
    "- Explain the chosen hyperparameters and justify their selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cb3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff757fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comparative Analysis ---\n",
    "# Compare performance of Logistic Regression and Random Forest\n",
    "\n",
    "models = ['Logistic Regression', 'Random Forest']\n",
    "accuracies = [accuracy_score(y_test, y_pred_logreg), accuracy_score(y_test, y_pred_rf)]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=models, y=accuracies)\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracies[0]:.2f}\")\n",
    "print(f\"Random Forest Accuracy: {accuracies[1]:.2f}\")\n",
    "\n",
    "# Print classification reports for both models\n",
    "print(\"\\nLogistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a658c",
   "metadata": {},
   "source": [
    "## 7. Comparative Analysis\n",
    "- Compare the performance of all models based on evaluation metrics.\n",
    "- Identify strengths and weaknesses of each model (e.g., speed, accuracy, interpretability).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conclusion & Comments ---\n",
    "# Summarize findings and provide final comments\n",
    "\n",
    "print(\"\\n--- Conclusion & Comments ---\")\n",
    "if accuracies[1] > accuracies[0]:\n",
    "    print(\"Random Forest outperformed Logistic Regression in sentiment classification.\")\n",
    "else:\n",
    "    print(\"Logistic Regression outperformed Random Forest in sentiment classification.\")\n",
    "\n",
    "print(\"Both models achieved reasonable accuracy. For further improvement, consider:\")\n",
    "print(\"- Using advanced NLP models (e.g., BERT, LSTM)\")\n",
    "print(\"- More feature engineering (e.g., sentiment lexicons, n-grams)\")\n",
    "print(\"- Hyperparameter tuning and cross-validation\")\n",
    "print(\"- Addressing class imbalance if present\")\n",
    "\n",
    "print(\"Regular monitoring of product reviews can help businesses respond to customer feedback and improve products.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Comments & Suggestions for Future Work ---\n",
    "# Provide suggestions for further analysis and improvements\n",
    "\n",
    "print(\"\\n--- Suggestions for Future Work ---\")\n",
    "print(\"1. Try deep learning models for better accuracy.\")\n",
    "print(\"2. Use more sophisticated text preprocessing (lemmatization, stemming).\")\n",
    "print(\"3. Explore unsupervised sentiment clustering.\")\n",
    "print(\"4. Visualize sentiment trends over time.\")\n",
    "print(\"5. Integrate with real-time review monitoring systems.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59b624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf254f52",
   "metadata": {},
   "source": [
    "## 8. Conclusion & Comments\n",
    "- Summarize the findings of the project.\n",
    "- Provide insights into the challenges faced during data preprocessing, model training, and evaluation.\n",
    "- Highlight key lessons learned.\n",
    "- Add clear and concise comments to the code for each step of the project.\n",
    "- Highlight key results, visualizations, and model comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464bee8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085e0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecea79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
